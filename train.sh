export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"

# TORCH_DISTRIBUTED_DEBUG=DETAIL TORCH_USE_CUDA_DSA=1 CUDA_LAUNCH_BLOCKING=1 RANK=0 LOCAL_RANK=0 WORLD_SIZE=1 MASTER_ADDR=127.0.0.1 MASTER_PORT=29502 python -m debugpy --listen 0.0.0.0:8106 --wait-for-client training/launch.py --config default_pm
TORCH_DISTRIBUTED_DEBUG=DETAIL TORCH_USE_CUDA_DSA=1 CUDA_LAUNCH_BLOCKING=1 RANK=0 LOCAL_RANK=0 WORLD_SIZE=1 MASTER_ADDR=127.0.0.1 MASTER_PORT=29502 python training/launch.py --config default_pm